{"metadata":{"dataExplorerConfig":{},"bento_stylesheets":{"bento/extensions/flow/main.css":true,"bento/extensions/kernel_selector/main.css":true,"bento/extensions/kernel_ui/main.css":true,"bento/extensions/new_kernel/main.css":true,"bento/extensions/system_usage/main.css":true,"bento/extensions/theme/main.css":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"last_server_session_id":"f09a260a-aacb-4a25-be07-fd346854a748","last_kernel_id":"9380f0fd-e775-4bb5-af67-b45cbb4ccaf0","last_base_url":"https://bento.edge.x2p.facebook.net/","last_msg_id":"0e5a3501-b8521d4333c726d6ea3cda08_3846","outputWidgetContext":{},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\n\n# find a clean device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# torch.backends.cudnn.benchmark = True\ntorch.cuda.set_device(device)\ntorch.cuda.empty_cache()\n","metadata":{"originalKey":"42a03f39-cfe8-486f-a08c-7f47ad8e8655","showInput":true,"requestMsgId":"42a03f39-cfe8-486f-a08c-7f47ad8e8655","executionStartTime":1690473701893,"executionStopTime":1690473701913,"id":"VXgzExifvQAb","outputId":"65fa1ac4-eeab-43a3-a7a8-a21a0c50a008","execution":{"iopub.status.busy":"2023-09-13T13:01:21.372464Z","iopub.execute_input":"2023-09-13T13:01:21.372991Z","iopub.status.idle":"2023-09-13T13:01:25.931244Z","shell.execute_reply.started":"2023-09-13T13:01:21.372953Z","shell.execute_reply":"2023-09-13T13:01:25.929556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport torchvision\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import Subset, DataLoader\nimport torch.nn.functional as tF\nimport torch.nn as nn\nimport time\n\nimport matplotlib.pyplot as plt\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n\n    def forward(self, x):\n        out = F.relu(self.conv1(x))\n        out = self.conv2(out)\n        out = F.relu(out + self.shortcut(x))\n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = F.relu(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\n\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 5, 3, 1)\n        self.conv2 = nn.Conv2d(5, 6, 4, 1)\n        self.conv3 = nn.Conv2d(6, 7, 3, 1)\n        self.fc1 = nn.Linear(1*1*7, 10)\n\n    def forward(self, x):\n        x = tF.relu(self.conv1(x))\n        x = tF.max_pool2d(x, 2, 2)\n        x = tF.relu(self.conv2(x))\n        x = tF.max_pool2d(x, 2, 2)\n        x = tF.relu(self.conv3(x))\n        x = tF.max_pool2d(x, 2, 2)\n        x = x.view(-1, 1*1*7)\n        x = self.fc1(x)\n        return tF.log_softmax(x, dim=1)\n\nclass MLP(torch.nn.Module):\n    def __init__(self, input_dim, output_dim, hidden1, hidden2):\n        super(MLP, self).__init__()\n        self.input_dim = input_dim\n        # self.linear = torch.nn.Linear(input_dim, hidden)\n        # self.fc = torch.nn.Linear(hidden, output_dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(input_dim, hidden1),\n            nn.ReLU(),\n            nn.Linear(hidden1, hidden2),\n            nn.ReLU(),\n            nn.Linear(hidden2, output_dim),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        x = x.view(-1, self.input_dim)\n        # x = self.linear(x)\n        outputs = self.mlp(x)\n        return tF.log_softmax(outputs, dim=1)\n\nclass MLP2(torch.nn.Module):\n    def __init__(self, input_dim, output_dim, hidden1, hidden2):\n        super(MLP2, self).__init__()\n        self.input_dim = input_dim\n        # self.linear = torch.nn.Linear(input_dim, hidden)\n        # self.fc = torch.nn.Linear(hidden, output_dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(input_dim, hidden1),\n            nn.ReLU(),\n            nn.Linear(hidden1, output_dim),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        x = x.view(-1, self.input_dim)\n        # x = self.linear(x)\n        outputs = self.mlp(x)\n        return tF.log_softmax(outputs, dim=1)","metadata":{"originalKey":"a27863c7-a12f-4741-8b5b-264a7d31d0eb","showInput":true,"customInput":null,"requestMsgId":"a27863c7-a12f-4741-8b5b-264a7d31d0eb","executionStartTime":1690474907709,"executionStopTime":1690474909017,"id":"3KNGuR6XvQAc","execution":{"iopub.status.busy":"2023-09-13T13:01:25.936906Z","iopub.execute_input":"2023-09-13T13:01:25.937475Z","iopub.status.idle":"2023-09-13T13:01:26.458429Z","shell.execute_reply.started":"2023-09-13T13:01:25.937445Z","shell.execute_reply":"2023-09-13T13:01:26.457354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transforms_normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\ntransform_list = [transforms.ToTensor(), transforms_normalize]\ntransformer = transforms.Compose(transform_list)\ntrainset = datasets.MNIST(root='/tmp/', train=True, download=True, transform=transforms.ToTensor())\ntestset = datasets.MNIST(root='/tmp/', train=False, download=True, transform=transforms.ToTensor())\ntrainloader = DataLoader(\n    dataset=trainset,\n    batch_size=128,\n    shuffle=True)\n\ntestloader = DataLoader(\n    dataset=testset,\n    batch_size=128,\n    shuffle=True)\n# repr in [PMatDense, PMatBlockDiag, PMatImplicit, PMatDiag, PMatEKFAC, PMatKFAC, PMatQuasiDiag]\ninput_dim = 784\n","metadata":{"originalKey":"69cc145d-c3cd-46be-873c-04b21a685c32","showInput":true,"customInput":null,"requestMsgId":"f127b422-9263-462c-8cca-fec5ecadd54e","executionStartTime":1693593953965,"executionStopTime":1693593953966,"id":"hUOkDzXTvQAc","outputId":"9469117b-0a72-4393-c566-f9fa221e36cd","execution":{"iopub.status.busy":"2023-09-12T22:16:16.629117Z","iopub.execute_input":"2023-09-12T22:16:16.629480Z","iopub.status.idle":"2023-09-12T22:16:17.798186Z","shell.execute_reply.started":"2023-09-12T22:16:16.629450Z","shell.execute_reply":"2023-09-12T22:16:17.797208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getgrad(model: torch.nn.Module, grad_dict: dict, step_iter=0):\n    if step_iter == 0:\n        for name, mod in model.named_modules():\n            if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.Linear):\n                # print(mod.weight.grad.data.size())\n                # print(mod.weight.data.size())\n                grad_dict[name] = [mod.weight.grad.data.cpu().reshape(-1).numpy()]\n    else:\n        for name, mod in model.named_modules():\n            if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.Linear):\n                grad_dict[name].append(mod.weight.grad.data.cpu().reshape(-1).numpy())\n\n    return grad_dict\n\n\ndef caculate_fr_zico(grad_dict, theta_dict, losses, theta_dict_copy=None):\n    \"\"\"Use implementation based on zico because the module names of search spaces in the\n    CV benchmark are different from the ads search space.\n    \"\"\"\n    allgrad_array = None\n    for i, modname in enumerate(grad_dict.keys()):\n        grad_dict[modname] = np.array(grad_dict[modname])\n    per_sample_mean_abs = np.zeros(4)\n    nsr_mean_sum_abs = 0\n    nsr_mean_sum_mean = 0\n    nsr_mean_sum_std = 0\n    fr_mean_sum_abs = 0\n    mean_abs_grad_value, std_grad_value = 0, 0\n    per_sample_prod_grad = np.zeros(4)\n\n    for j, modname in enumerate(grad_dict.keys()):\n        nsr_std = np.std(grad_dict[modname], axis=0)\n        nonzero_idx = np.nonzero(nsr_std)[0]\n        nsr_mean_abs = np.mean(np.abs(grad_dict[modname]), axis=0)\n        temp = np.mean(np.abs(grad_dict[modname])[:, nonzero_idx], axis=1)\n\n        tmpsum = np.sum(nsr_mean_abs[nonzero_idx] / nsr_std[nonzero_idx])\n        mean_abs_grad, std_grad = np.sum(nsr_mean_abs[nonzero_idx]), np.sum(\n            nsr_std[nonzero_idx]\n        )\n        if tmpsum == 0:\n            pass\n        else:\n            nsr_mean_sum_abs += np.log(tmpsum)\n            nsr_sum_grad = np.sum(\n                np.sum(grad_dict[modname], axis=0) * theta_dict[-1][modname]\n            )\n            fr_mean_sum_abs += np.abs(nsr_sum_grad)\n            mean_abs_grad_value += mean_abs_grad\n            std_grad_value += std_grad\n\n    return (\n        nsr_mean_sum_abs,\n        nsr_mean_sum_abs + np.log(fr_mean_sum_abs + 1e-5),\n        mean_abs_grad_value,\n        std_grad_value,\n    )","metadata":{"originalKey":"4ae01ff6-0d1a-430d-9029-7edb3dcf9107","showInput":true,"customInput":null,"requestMsgId":"183b9e00-d728-4754-9b38-f12b9c780594","executionStartTime":1693594636608,"executionStopTime":1693594638734,"id":"woz_JAhFvQAd","execution":{"iopub.status.busy":"2023-09-12T22:16:19.479481Z","iopub.execute_input":"2023-09-12T22:16:19.479830Z","iopub.status.idle":"2023-09-12T22:16:19.493915Z","shell.execute_reply.started":"2023-09-12T22:16:19.479801Z","shell.execute_reply":"2023-09-12T22:16:19.492902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\nimport numpy as np\n\neval_batch = 4\nepoch = 3\nlast_loss, train_losses, last_scores, cur_losses, mean_abs_grad_list, std_grad_list, zico_list = [], [], [], [], [], [], []\nlast_scores01, cur_losses01, mean_abs_grad_list01, std_grad_list01, zico_list01 = [], [], [], [], []\nlast_scores02, cur_losses02, mean_abs_grad_list02, std_grad_list02, zico_list02 = [], [], [], [], []\n\nbatch_10 = 48\nbatch_20 = 48 * 4\nfor k1 in range(25):\n    for k2 in range(25):\n        # convnet = ConvNet().to(device)\n        convnet = MLP2(input_dim, 10, (k1 + 1) * 2, (k2 + 1) * 2).to(device)\n        convnet2 = MLP2(input_dim, 10, (k1 + 1) * 2, (k2 + 1) * 2).to(device)\n        optimizer = torch.optim.SGD(convnet.parameters(), lr=0.2)\n        loss_fn = tF.cross_entropy\n        # Here, we use enumerate(training_loader) instead of\n        # iter(training_loader) so that we can track the batch\n        # index and do some intra-epoch reporting\n        num_param = 0\n        for d in convnet.state_dict():\n            num_param += convnet.state_dict()[d].flatten().size()[0]\n\n        # ini_theta = []\n        # for i in convnet.state_dict():\n        #     ini_theta.append(convnet.state_dict()[i].flatten())\n        # ini_theta = torch.concatenate(ini_theta)\n        train_loss, train_loss01, train_loss02 = [], [], []\n        grad_dict, grad_dict01, grad_dict02 = {}, {}, {}\n        scores = torch.zeros(num_param).to(device)\n        scores01 = torch.zeros(num_param).to(device)\n        scores02 = torch.zeros(num_param).to(device)\n        for e in range(epoch):\n            for i, data in enumerate(trainloader):\n                # Every data instance is an input + label pair\n                inputs, labels = data\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                # Zero your gradients for every batch!\n                optimizer.zero_grad()\n\n                # Make predictions for this batch\n                outputs = convnet(inputs)\n\n                # Compute the loss and its gradients\n                loss = loss_fn(outputs, labels)\n                loss.backward()\n                # Adjust learning weights\n                optimizer.step()\n                losses, theta_list = [], []\n                losses01, theta_list01 = [], []\n                losses02, theta_list02 = [], []\n                if e == 0 and i < eval_batch: # only train with eval_batch batches\n                    # copy all weights to a separate model\n                    convnet2.load_state_dict(copy.deepcopy(convnet.state_dict()))\n                    outputs2 = convnet2(inputs)\n                    loss2 = loss_fn(outputs2, labels)\n                    loss2.backward()\n                    grad = []\n                    for name, param in convnet2.named_parameters():\n                        if param.requires_grad:\n                            grad.append(param.grad.flatten())\n                    scores += torch.concatenate(grad)\n\n                    grad_dict= getgrad(convnet2, grad_dict,i)\n                    losses.append(loss2.item())\n\n                    theta_dict = {}\n                    for name, mod in convnet2.named_modules():\n                        if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.Linear):\n                            theta_dict[name] = mod.weight.data.cpu().reshape(-1).numpy()\n                    theta_list.append(theta_dict)\n\n                    if i == eval_batch - 1:\n                        cur_losses.append(loss.item())\n                        score1, score2, mean_abs_grad, std_grad = caculate_fr_zico(grad_dict, theta_list, np.array(losses))\n                        mean_abs_grad_list.append(mean_abs_grad)\n                        std_grad_list.append(std_grad)\n                        zico_list.append(score1)\n\n                        theta = []\n                        for d in convnet.state_dict():\n                            theta.append(convnet.state_dict()[d].flatten())\n                        theta = torch.concatenate(theta)\n                        score = torch.log((scores * theta).sum()**2)\n\n                if e == 0 and i >= batch_10 and i < batch_10 + eval_batch: # only train with eval_batch batches after 24 batches (10%) warm-up\n\n                    # copy all weights to a separate model\n                    convnet2.load_state_dict(copy.deepcopy(convnet.state_dict()))\n                    outputs2 = convnet2(inputs)\n                    loss2 = loss_fn(outputs2, labels)\n                    loss2.backward()\n                    grad = []\n                    for name, param in convnet2.named_parameters():\n                        if param.requires_grad:\n                            grad.append(param.grad.flatten())\n                    scores01 += torch.concatenate(grad)\n\n                    grad_dict01= getgrad(convnet2, grad_dict01, i - batch_10)\n                    losses01.append(loss2.item())\n\n                    theta_dict = {}\n                    for name, mod in convnet2.named_modules():\n                        if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.Linear):\n                            theta_dict[name] = mod.weight.data.cpu().reshape(-1).numpy()\n                    theta_list01.append(theta_dict)\n\n                    if i == batch_10 + eval_batch - 1:\n                        cur_losses01.append(loss2.item())\n                        score1, score2, mean_abs_grad, std_grad = caculate_fr_zico(grad_dict01, theta_list01, np.array(losses01))\n                        mean_abs_grad_list01.append(mean_abs_grad)\n                        std_grad_list01.append(std_grad)\n                        zico_list01.append(score1)\n\n                        theta01 = []\n                        for c in convnet.state_dict():\n                            theta01.append(convnet.state_dict()[c].flatten())\n                        theta01 = torch.concatenate(theta01)\n                        score01 = torch.log((scores01 * theta01).sum()**2)\n\n                if e == 0 and i >= batch_20 and i < batch_20 + eval_batch: # only train with eval_batch batches after 24 batches (20%) warm-up\n\n                    # copy all weights to a separate model\n                    convnet2.load_state_dict(copy.deepcopy(convnet.state_dict()))\n                    outputs2 = convnet2(inputs)\n                    loss2 = loss_fn(outputs2, labels)\n                    loss2.backward()\n                    grad = []\n                    for name, param in convnet2.named_parameters():\n                        if param.requires_grad:\n                            grad.append(param.grad.flatten())\n                    scores02 += torch.concatenate(grad)\n\n                    grad_dict02= getgrad(convnet2, grad_dict02, i - batch_20)\n                    losses02.append(loss2.item())\n\n                    theta_dict = {}\n                    for name, mod in convnet2.named_modules():\n                        if isinstance(mod, nn.Conv2d) or isinstance(mod, nn.Linear):\n                            theta_dict[name] = mod.weight.data.cpu().reshape(-1).numpy()\n                    theta_list02.append(theta_dict)\n\n                    if i == batch_20 + eval_batch - 1:\n                        cur_losses02.append(loss2.item())\n                        score1, score2, mean_abs_grad, std_grad = caculate_fr_zico(grad_dict02, theta_list02, np.array(losses02))\n                        mean_abs_grad_list02.append(mean_abs_grad)\n                        std_grad_list02.append(std_grad)\n                        zico_list02.append(score1)\n\n                        theta02 = []\n                        for c in convnet.state_dict():\n                            theta02.append(convnet.state_dict()[c].flatten())\n                        theta02 = torch.concatenate(theta02)\n                        score02 = torch.log((scores02 * theta02).sum()**2)\n\n                if e == epoch - 1:\n                    train_loss.append(loss.item())\n\n\n        # evaluate accuracy at end of training\n        convnet.eval()\n        val_loss = []\n        for i, data in enumerate(testloader):\n            # Every data instance is an input + label pair\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n            # Make predictions for this batch\n            outputs = convnet(inputs)\n            loss = loss_fn(outputs, labels)\n            val_loss.append(loss.item())\n\n        # F_dense = F.get_dense_tensor()\n        # score = zero_score(convnet, trainloader, repr)\n        last_loss.append(sum(val_loss) / len(val_loss))\n        train_losses.append(sum(train_loss) / len(train_loss))\n        last_scores.append(score.item())\n        last_scores01.append(score01.item())\n        last_scores02.append(score02.item())\n\n        print('step {}-{}: train loss: {}, val loss: {}, cur loss: {}, mean abs gradient: {}, std gradient: {}, zico: {}, score: {}'.format((k1 + 1) * 1, (k2 + 1) * 1, \\\n        sum(train_loss) / len(train_loss), sum(val_loss) / len(val_loss), cur_losses[-1], mean_abs_grad_list[-1], std_grad_list[-1], zico_list[-1], score))\n\n        print('step {}-{}: train loss: {}, val loss: {}, cur loss: {}, mean abs gradient (10%): {}, std gradient (10%): {}, zico (10%): {}, score (10%): {}'.format((k1 + 1) * 1, (k2 + 1) * 1, \\\n        sum(train_loss) / len(train_loss), sum(val_loss) / len(val_loss), cur_losses01[-1], mean_abs_grad_list01[-1], std_grad_list01[-1], zico_list01[-1], score01))\n","metadata":{"originalKey":"59797910-fa4d-4d23-80a1-6bd76f878655","showInput":true,"customInput":null,"requestMsgId":"0fa6afd7-5a4a-4056-8a52-fdec4e52f842","executionStartTime":1693593531036,"executionStopTime":1693593531290,"id":"6_X8qdjovQAd","outputId":"a2c4d7f2-da9d-4444-f976-ecd16582a3e5","execution":{"iopub.status.busy":"2023-09-12T22:16:21.955360Z","iopub.execute_input":"2023-09-12T22:16:21.955819Z","iopub.status.idle":"2023-09-13T01:57:46.450998Z","shell.execute_reply.started":"2023-09-12T22:16:21.955780Z","shell.execute_reply":"2023-09-13T01:57:46.450005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy\nfrom scipy import stats\n\nfig, ax = plt.subplots(3, 3, sharex=\"col\", sharey=\"row\", figsize=(20, 10))\nfig.subplots_adjust(hspace=0.3, wspace=0.1)\n\nplt.style.use(\"seaborn-white\")\n# data = [cur_losses, np.exp(last_scores), mean_abs_grad_list, 220 - np.log(std_grad_list) * 50]\ndata = [\n    [cur_losses, last_scores, mean_abs_grad_list, std_grad_list],\n    [cur_losses01, last_scores01, mean_abs_grad_list01, std_grad_list01],\n    [cur_losses02, last_scores02, mean_abs_grad_list02, std_grad_list02],\n]\nloss_resutls = [last_loss, last_loss, last_loss]\nlabels = [\n    \"Current Training Loss\",\n    \"Fisher-Rao Norm\",\n    \"Mean Absolute Gradients\",\n    \"Standard Deviation of Gradients\",\n]\nfor i in range(3):\n    for j in range(3):\n        ax[i, j].scatter(data[i][j], loss_resutls[i])\n        ax[i, j].set_title(\n            \"Spearman's rho: {:.2f} \\n kendall's tau: {:.2f}\".format(\n                stats.spearmanr(data[i][j], loss_resutls[i]).correlation,\n                stats.kendalltau(data[i][j], loss_resutls[i]).correlation,\n            ),\n            fontsize=16,\n        )\n        if j == 0:\n            ax[i, j].set_ylabel(\"Test Loss\", fontsize=18)\n        if i == 2:\n            ax[i, j].set_xlabel(labels[j], fontsize=18)\nplt.savefig('correlation_test_train_loss_all_warmup-2Layers.pdf', bbox_inches=\"tight\")\nplt.savefig('correlation_test_train_loss_all_warmup-2Layers.png', bbox_inches=\"tight\")","metadata":{"originalKey":"e1a703c5-4aeb-4b62-86ff-5cf182291ad4","showInput":true,"customInput":null,"requestMsgId":"0ff9111c-f224-41d5-b24e-c6da98aa43fb","executionStartTime":1693593511518,"executionStopTime":1693593511767,"id":"HXv-0yPhvQAe","execution":{"iopub.status.busy":"2023-09-13T01:57:48.572774Z","iopub.status.idle":"2023-09-13T01:57:48.573513Z","shell.execute_reply.started":"2023-09-13T01:57:48.573262Z","shell.execute_reply":"2023-09-13T01:57:48.573288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy\n\nfig, ax = plt.subplots(2, 3, sharex='col', sharey='row', figsize=(20,10))\nfig.subplots_adjust(hspace=0.2, wspace=0.1)\n\nplt.style.use('seaborn-white')\n# data = [cur_losses, np.exp(last_scores), mean_abs_grad_list, 220 - np.log(std_grad_list) * 50]\ndata = [cur_losses, last_scores, mean_abs_grad_list, std_grad_list]\nloss_resutls = [train_losses, last_loss]\nlabels = ['Current Training Loss', 'Fisher-Rao Norm', \"Mean Absolute Gradients\", \"Standard Deviation of Gradients\"]\nfor i in range(2):\n    for j in range(3):\n        ax[i, j].scatter(data[j], loss_resutls[i])\n        ax[i, j].set_title(\"Spearman's rho: {:.2f} \\n kendall's tau: {:.2f}\".format(\n            stats.spearmanr(data[j], loss_resutls[i]).correlation,\n            stats.kendalltau(data[j], loss_resutls[i]).correlation), fontsize=16)\n        if j == 0:\n            if i == 0:\n                ax[i, j].set_ylabel('Training Loss (Last Epoch)', fontsize=18)\n            else:\n                ax[i, j].set_ylabel('Test Loss', fontsize=18)\n        if i == 1:\n            ax[i, j].set_xlabel(labels[j], fontsize=18)\nplt.savefig('correlation_test_train_loss_0warmup-2Layers.pdf', bbox_inches=\"tight\")\nplt.savefig('correlation_test_train_loss_0warmup-2Layers.png', bbox_inches=\"tight\")\n","metadata":{"originalKey":"5958a998-d117-4dab-8a82-74ae42e4542a","showInput":true,"customInput":null,"id":"CbU1AsLTvQAe","execution":{"iopub.status.busy":"2023-09-13T01:57:48.574895Z","iopub.status.idle":"2023-09-13T01:57:48.575597Z","shell.execute_reply.started":"2023-09-13T01:57:48.575356Z","shell.execute_reply":"2023-09-13T01:57:48.575380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(2, 3, sharex='col', sharey='row', figsize=(20,10))\nfig.subplots_adjust(hspace=0.2, wspace=0.1)\n\nplt.style.use('seaborn-white')\n# data = [cur_losses, np.exp(last_scores), mean_abs_grad_list, 220 - np.log(std_grad_list) * 50]\ndata = [cur_losses01, last_scores01, mean_abs_grad_list01, std_grad_list01]\nloss_resutls = [train_losses, last_loss]\nlabels = ['Current Training Loss', 'Fisher-Rao Norm', \"Mean Absolute Gradients\", \"Standard Deviation of Gradients\"]\nfor i in range(2):\n    for j in range(3):\n        ax[i, j].scatter(data[j], loss_resutls[i])\n        ax[i, j].set_title(\"Spearman's rho: {:.2f} \\n kendall's tau: {:.2f}\".format(\n            stats.spearmanr(data[j], loss_resutls[i]).correlation,\n            stats.kendalltau(data[j], loss_resutls[i]).correlation), fontsize=16)\n        if j == 0:\n            if i == 0:\n                ax[i, j].set_ylabel('Training Loss (Last Epoch)', fontsize=18)\n            else:\n                ax[i, j].set_ylabel('Test Loss', fontsize=18)\n        if i == 1:\n            ax[i, j].set_xlabel(labels[j], fontsize=18)\nplt.savefig('correlation_test_train_loss_10warmup-2Layers.pdf', bbox_inches=\"tight\")\nplt.savefig('correlation_test_train_loss_10warmup-2Layers.png', bbox_inches=\"tight\")","metadata":{"id":"nbRfQ6QK9lTy","execution":{"iopub.status.busy":"2023-09-13T01:57:48.576910Z","iopub.status.idle":"2023-09-13T01:57:48.577601Z","shell.execute_reply.started":"2023-09-13T01:57:48.577362Z","shell.execute_reply":"2023-09-13T01:57:48.577385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy\n\nfig, ax = plt.subplots(2, 3, sharex='col', sharey='row', figsize=(20,10))\nfig.subplots_adjust(hspace=0.2, wspace=0.1)\n\nplt.style.use('seaborn-white')\n# data = [cur_losses, np.exp(last_scores), mean_abs_grad_list, 220 - np.log(std_grad_list) * 50]\ndata = [cur_losses02, last_scores02, mean_abs_grad_list02, std_grad_list02]\nloss_resutls = [train_losses, last_loss]\nlabels = ['Current Training Loss', 'Fisher-Rao Norm', \"Mean Absolute Gradients\", \"Standard Deviation of Gradients\"]\nfor i in range(2):\n    for j in range(3):\n        ax[i, j].scatter(data[j], loss_resutls[i])\n        ax[i, j].set_title(\"Spearman's rho: {:.2f} \\n kendall's tau: {:.2f}\".format(\n            stats.spearmanr(data[j], loss_resutls[i]).correlation,\n            stats.kendalltau(data[j], loss_resutls[i]).correlation), fontsize=16)\n        if j == 0:\n            if i == 0:\n                ax[i, j].set_ylabel('Training Loss (Last Epoch)', fontsize=18)\n            else:\n                ax[i, j].set_ylabel('Test Loss', fontsize=18)\n        if i == 1:\n            ax[i, j].set_xlabel(labels[j], fontsize=18)\nplt.savefig('correlation_test_train_loss_30warmup-2Layers.pdf', bbox_inches=\"tight\")\nplt.savefig('correlation_test_train_loss_30warmup-2Layers.png', bbox_inches=\"tight\")\n","metadata":{"id":"_CZhwOO39q9n","execution":{"iopub.status.busy":"2023-09-13T01:57:48.578882Z","iopub.status.idle":"2023-09-13T01:57:48.579579Z","shell.execute_reply.started":"2023-09-13T01:57:48.579336Z","shell.execute_reply":"2023-09-13T01:57:48.579360Z"},"trusted":true},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}